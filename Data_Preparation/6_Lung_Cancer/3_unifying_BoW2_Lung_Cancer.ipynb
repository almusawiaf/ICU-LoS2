{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_pickle(thePath):\n",
    "    with open(thePath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def save_pickle(L, given_path, file_name):\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(given_path):\n",
    "        os.makedirs(given_path)\n",
    "        print(f'\\tDirectory created: {given_path}')\n",
    "    \n",
    "    # Save the list as a pickle file\n",
    "    print(f'\\tSaving to {given_path}/{file_name}.pkl')\n",
    "    with open(os.path.join(given_path, f'{file_name}.pkl'), 'wb') as file:\n",
    "        pickle.dump(L, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading structured and unstructured data...\n",
    "\n",
    "the_path    = '../Data'\n",
    "\n",
    "X_B  = load_pickle(f'{the_path}/structured/Lung_Cancer/X_B.pkl')\n",
    "# X_D  = load_pickle(f'{the_path}/structured/Lung_Cancer/X_D.pkl')\n",
    "X_L  = load_pickle(f'{the_path}/structured/Lung_Cancer/X_L.pkl')\n",
    "X_M  = load_pickle(f'{the_path}/structured/Lung_Cancer/X_M.pkl')\n",
    "X_P  = load_pickle(f'{the_path}/structured/Lung_Cancer/X_P.pkl')\n",
    "X_S  = load_pickle(f'{the_path}/structured/Lung_Cancer/X_S.pkl')\n",
    "VIY  = load_pickle(f'{the_path}/structured/Lung_Cancer/VIY.pkl')\n",
    "\n",
    "# ... unstructured data\n",
    "model_name  = 'sci_sm' \n",
    "num_Tokens  = 2000\n",
    "org_path    = f'{the_path}/unstructured/emb/BoW/ALL_first_last_{model_name}_{num_Tokens}.csv'\n",
    "\n",
    "df_emb = pd.read_csv(org_path)\n",
    "\n",
    "MIMIC_Path = '/lustre/home/almusawiaf/PhD_Projects/MIMIC_resources'\n",
    "df_75_features = pd.read_csv(f'{MIMIC_Path}/ICU_patient_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.30744e+05, 3.02370e+00],\n",
       "       [1.08732e+05, 2.18970e+00],\n",
       "       [1.37006e+05, 3.18200e+00],\n",
       "       ...,\n",
       "       [1.95348e+05, 1.21212e+01],\n",
       "       [1.44869e+05, 2.90750e+00],\n",
       "       [1.29743e+05, 4.12270e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating new X_VI\n",
    "X_VI2 = []\n",
    "for v,_, i in VIY:\n",
    "    X_VI2.append([int(v), i])\n",
    "\n",
    "X_VI2 = np.array(X_VI2)\n",
    "X_VI2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_HADM_ID = np.intersect1d(df_emb['HADM_ID'], X_VI2[:, 0])\n",
    "common_HADM_ID = np.sort(common_HADM_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(709, 2) (684, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.00085e+05, 1.14000e+00],\n",
       "       [1.00271e+05, 1.96060e+00],\n",
       "       [1.00283e+05, 3.86510e+00],\n",
       "       ...,\n",
       "       [1.99577e+05, 1.31443e+01],\n",
       "       [1.99616e+05, 9.00250e+00],\n",
       "       [1.99889e+05, 8.96050e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mask for the original array\n",
    "mask = np.in1d(X_VI2[:,0], common_HADM_ID)\n",
    "\n",
    "# Create a new array with the common HADM_ID\n",
    "new_X_VI2 = X_VI2[mask]\n",
    "\n",
    "# Sort the new array based on the common HADM_ID\n",
    "idx = np.argsort(new_X_VI2[:, 0])\n",
    "new_X_VI2 = new_X_VI2[idx]\n",
    "\n",
    "print(X_VI2.shape, new_X_VI2.shape)\n",
    "new_X_VI2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_B2 = X_B[mask][idx]\n",
    "X_L2 = X_L[mask][idx]\n",
    "X_M2 = X_M[mask][idx]\n",
    "X_P2 = X_P[mask][idx]\n",
    "X_S2 = X_S[mask][idx]\n",
    "\n",
    "VIY2 = VIY[mask][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for the original DataFrame\n",
    "df = df_emb.copy()\n",
    "mask_df = df['HADM_ID'].isin(common_HADM_ID)\n",
    "\n",
    "# Create a new DataFrame with the common HADM_ID\n",
    "new_df = df[mask_df].copy()\n",
    "\n",
    "# Sort the new DataFrame based on the common HADM_ID\n",
    "new_df = new_df.set_index('HADM_ID')\n",
    "new_df = new_df.loc[common_HADM_ID]\n",
    "new_df = new_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>ADM_ELECTIVE</th>\n",
       "      <th>ADM_EMERGENCY</th>\n",
       "      <th>ADM_URGENT</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Systolic Blood Pressure</th>\n",
       "      <th>Diastolic Blood Pressure</th>\n",
       "      <th>Respiratory Rate</th>\n",
       "      <th>Pulse Oximetry</th>\n",
       "      <th>...</th>\n",
       "      <th>Inspired O2 Fraction</th>\n",
       "      <th>BUN</th>\n",
       "      <th>Anion Gap</th>\n",
       "      <th>INR</th>\n",
       "      <th>GENDER_M</th>\n",
       "      <th>GENDER_F</th>\n",
       "      <th>AGE_AGE middle adult</th>\n",
       "      <th>AGE_AGE senior</th>\n",
       "      <th>AGE_Other</th>\n",
       "      <th>AGE_Unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57781</th>\n",
       "      <td>199993</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57782</th>\n",
       "      <td>199994</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57783</th>\n",
       "      <td>199995</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57784</th>\n",
       "      <td>199998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57785</th>\n",
       "      <td>199999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57786 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       HADM_ID  ADMISSION_TYPE  ADM_ELECTIVE  ADM_EMERGENCY  ADM_URGENT  \\\n",
       "0       100001             1.0           0.0            1.0         0.0   \n",
       "1       100003             1.0           0.0            1.0         0.0   \n",
       "2       100006             1.0           0.0            1.0         0.0   \n",
       "3       100007             1.0           0.0            1.0         0.0   \n",
       "4       100009             1.0           0.0            1.0         0.0   \n",
       "...        ...             ...           ...            ...         ...   \n",
       "57781   199993             1.0           0.0            1.0         0.0   \n",
       "57782   199994             1.0           0.0            1.0         0.0   \n",
       "57783   199995             1.0           0.0            1.0         0.0   \n",
       "57784   199998             1.0           0.0            1.0         0.0   \n",
       "57785   199999             1.0           0.0            1.0         0.0   \n",
       "\n",
       "       Heart Rate  Systolic Blood Pressure  Diastolic Blood Pressure  \\\n",
       "0           122.0                    192.0                     100.0   \n",
       "1            71.0                     83.0                      36.0   \n",
       "2            87.0                    122.0                      65.0   \n",
       "3            87.0                    122.0                      65.0   \n",
       "4            80.0                    117.0                      47.0   \n",
       "...           ...                      ...                       ...   \n",
       "57781        87.0                    122.0                      65.0   \n",
       "57782        87.0                    122.0                      65.0   \n",
       "57783        87.0                    122.0                      65.0   \n",
       "57784        87.0                    122.0                      65.0   \n",
       "57785        81.0                    128.0                      54.0   \n",
       "\n",
       "       Respiratory Rate  Pulse Oximetry  ...  Inspired O2 Fraction   BUN  \\\n",
       "0                  14.0           100.0  ...                 100.0  42.0   \n",
       "1                  11.0            96.0  ...                 100.0  49.0   \n",
       "2                  18.0            98.0  ...                 100.0  19.0   \n",
       "3                  18.0            98.0  ...                 100.0  19.0   \n",
       "4                  21.0           100.0  ...                 100.0  13.0   \n",
       "...                 ...             ...  ...                   ...   ...   \n",
       "57781              18.0            98.0  ...                 100.0  19.0   \n",
       "57782              18.0            98.0  ...                 100.0  19.0   \n",
       "57783              18.0            98.0  ...                 100.0  19.0   \n",
       "57784              18.0            98.0  ...                 100.0  16.0   \n",
       "57785              15.0            94.0  ...                  60.0  18.0   \n",
       "\n",
       "       Anion Gap  INR  GENDER_M  GENDER_F  AGE_AGE middle adult  \\\n",
       "0           20.0  1.3       0.0       1.0                   0.0   \n",
       "1           10.0  1.6       1.0       0.0                   1.0   \n",
       "2           13.0  1.3       0.0       1.0                   1.0   \n",
       "3           13.0  1.3       0.0       1.0                   0.0   \n",
       "4           12.0  1.2       1.0       0.0                   1.0   \n",
       "...          ...  ...       ...       ...                   ...   \n",
       "57781       13.0  1.3       1.0       0.0                   1.0   \n",
       "57782       13.0  1.3       0.0       1.0                   1.0   \n",
       "57783       13.0  1.3       1.0       0.0                   0.0   \n",
       "57784        8.0  1.3       1.0       0.0                   0.0   \n",
       "57785       10.0  1.3       1.0       0.0                   0.0   \n",
       "\n",
       "       AGE_AGE senior  AGE_Other  AGE_Unknown  \n",
       "0                 0.0        1.0          0.0  \n",
       "1                 0.0        0.0          0.0  \n",
       "2                 0.0        0.0          0.0  \n",
       "3                 1.0        0.0          0.0  \n",
       "4                 0.0        0.0          0.0  \n",
       "...               ...        ...          ...  \n",
       "57781             0.0        0.0          0.0  \n",
       "57782             0.0        0.0          0.0  \n",
       "57783             0.0        1.0          0.0  \n",
       "57784             1.0        0.0          0.0  \n",
       "57785             1.0        0.0          0.0  \n",
       "\n",
       "[57786 rows x 27 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df_75_features.copy()\n",
    "\n",
    "# ===============================================================================\n",
    "# 1. Convert all boolean columns to integer (True → 1, False → 0)\n",
    "df3 = df3.astype({col: int for col in df3.select_dtypes(include='bool').columns})\n",
    "# ===============================================================================\n",
    "# 2. label encoding for admission type\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the column\n",
    "df3['ADMISSION_TYPE'] = le.fit_transform(df3['ADMISSION_TYPE'])\n",
    "# ===============================================================================\n",
    "# 3. dropping unneeded cols\n",
    "df3.drop(columns=['Unnamed: 0', 'SUBJECT_ID', 'ADMITTIME', 'ICUSTAY_ID'], inplace=True)\n",
    "# ===============================================================================\n",
    "\n",
    "df3 = df3.groupby('HADM_ID').mean().reset_index()\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All HADM_IDs in df1 exist in df3.\n",
      "(684, 27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(684, 26)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensuring all ids exist in both tables.\n",
    "missing_hadm = set(new_df['HADM_ID']) - set(df3['HADM_ID'])\n",
    "\n",
    "if missing_hadm:\n",
    "    print(\"Missing HADM_IDs in df3:\", missing_hadm)\n",
    "else:\n",
    "    print(\"All HADM_IDs in df1 exist in df3.\")\n",
    "    \n",
    "# Filter df2 to keep only HADM_IDs present in df1\n",
    "df3 = df3[df3['HADM_ID'].isin(new_df['HADM_ID'])]\n",
    "\n",
    "# Sort df2 to match the order of HADM_ID in df1\n",
    "df3 = df3.set_index('HADM_ID').reindex(new_df['HADM_ID']).reset_index()\n",
    "\n",
    "# Verify dimensions\n",
    "print(df3.shape)  # Should be (42142, 31)\n",
    "\n",
    "df3.drop(columns=['HADM_ID'], inplace=True )\n",
    "F = df3.values\n",
    "F.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.51837718,  1.83212661, -1.71864983, ...,  0.9185128 ,\n",
       "        -0.22153951, -0.15964729],\n",
       "       [ 0.33952978, -0.54581381,  0.58185209, ..., -1.08871645,\n",
       "        -0.22153951, -0.15964729],\n",
       "       [ 0.33952978, -0.54581381,  0.58185209, ...,  0.9185128 ,\n",
       "        -0.22153951, -0.15964729],\n",
       "       ...,\n",
       "       [ 0.33952978, -0.54581381,  0.58185209, ...,  0.9185128 ,\n",
       "        -0.22153951, -0.15964729],\n",
       "       [ 0.33952978, -0.54581381,  0.58185209, ..., -1.08871645,\n",
       "        -0.22153951, -0.15964729],\n",
       "       [-1.51837718,  1.83212661, -1.71864983, ..., -1.08871645,\n",
       "        -0.22153951, -0.15964729]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# # Min-max scaling\n",
    "# scaler = MinMaxScaler()\n",
    "# min_max_scaled_features = scaler.fit_transform(F)\n",
    "# min_max_scaled_features[1]\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "F2 = scaler.fit_transform(F)\n",
    "F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(684, 26)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 547/547 [00:00<00:00, 1925.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 137/137 [00:00<00:00, 2041.08 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Helper Functions\n",
    "def classify_los_3_classes(los_list, th1, th2):\n",
    "    return [0 if los < th1 else 1 if th1 <= los <= th2 else 2 for los in los_list]\n",
    "\n",
    "\n",
    "def classify_los_binary(los_list, threshold):\n",
    "    return [1 if los > threshold else 0 for los in los_list]\n",
    "\n",
    "num_classes = '2_classes'\n",
    "\n",
    "if num_classes =='2_classes':\n",
    "    threshold = 7    \n",
    "    labels = list(classify_los_binary(VIY2[:, 2], threshold))\n",
    "    saving_path = f'{the_path}/XY_BoW/{num_classes}/BoW_{model_name}_{num_Tokens}_F_{threshold}_days_Lung_Cancer'\n",
    "else:\n",
    "    th1, th2 = 3, 7\n",
    "    labels = list(classify_los_3_classes(VIY2[:, 2], th1, th2))\n",
    "    saving_path = f'{the_path}/XY_BoW/{num_classes}/BoW_{model_name}_{num_Tokens}_F_{th1}_{th2}_days_Lung_Cancer'\n",
    "    \n",
    "    \n",
    "# Encode Labels\n",
    "visits = list(VIY2[:, 0])\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "E = new_df.drop(columns=['HADM_ID']).values\n",
    "\n",
    "# Ensure structured data is converted to lists (if needed)\n",
    "XM_list = X_M2.tolist()\n",
    "XS_list = X_S2.tolist()\n",
    "XB_list = X_B2.tolist()\n",
    "XL_list = X_L2.tolist()\n",
    "XP_list = X_P2.tolist()\n",
    "\n",
    "XF_list = F.tolist()\n",
    "\n",
    "# Perform the train-test split\n",
    "(train_labels, test_labels, \n",
    " train_visits, test_visits, \n",
    " train_XM, test_XM, \n",
    " train_XS, test_XS,\n",
    " train_XB, test_XB,\n",
    " train_XL, test_XL,\n",
    " train_XP, test_XP,\n",
    " train_XF, test_XF,\n",
    " train_E , test_E) = train_test_split(labels, visits, XM_list, XS_list, XB_list, XL_list, XP_list, XF_list, E,test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"label\": train_labels, \n",
    "                                   \"HADM_ID\": train_visits})\n",
    "\n",
    "train_dataset2 = Dataset.from_dict({\"label\": train_labels, \n",
    "                                   \"HADM_ID\": train_visits,\n",
    "                                   \"XM\": train_XM,\n",
    "                                   \"XS\": train_XS,\n",
    "                                   \"XB\": train_XB,\n",
    "                                   \"XL\": train_XL,\n",
    "                                   \"XP\": train_XP,\n",
    "                                   \"XF\": train_XF,\n",
    "                                   \"XE\": train_E})\n",
    "\n",
    "test_dataset  = Dataset.from_dict({\"label\": test_labels,  \n",
    "                                   \"HADM_ID\": test_visits})\n",
    "\n",
    "test_dataset2  = Dataset.from_dict({\"label\": test_labels,  \n",
    "                                   \"HADM_ID\": test_visits,\n",
    "                                   \"XM\": test_XM,\n",
    "                                   \"XS\": test_XS,\n",
    "                                   \"XB\": test_XB,\n",
    "                                   \"XL\": test_XL,\n",
    "                                   \"XP\": test_XP,\n",
    "                                   \"XF\": test_XF,\n",
    "                                   \"XE\": test_E})\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(saving_path, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "train_dataset2.save_to_disk(f\"{saving_path}/train_dataset_X\")\n",
    "test_dataset2.save_to_disk(f\"{saving_path}/test_dataset_X\")\n",
    "print(\"Datasets saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envBoW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
