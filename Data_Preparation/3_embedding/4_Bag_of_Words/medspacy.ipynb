{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /lustre/home/almusawiaf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /lustre/home/almusawiaf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E871] Error encountered in nlp.pipe with multiprocessing:\n\nTraceback (most recent call last):\n  File \"/lustre/home/almusawiaf/anaconda3/envs/envBoW3/lib/python3.10/site-packages/spacy/language.py\", line 2364, in _apply_pipes\n    byte_docs = [(doc.to_bytes(), doc._context, None) for doc in docs]\n  File \"/lustre/home/almusawiaf/anaconda3/envs/envBoW3/lib/python3.10/site-packages/spacy/language.py\", line 2364, in <listcomp>\n    byte_docs = [(doc.to_bytes(), doc._context, None) for doc in docs]\n  File \"/lustre/home/almusawiaf/anaconda3/envs/envBoW3/lib/python3.10/site-packages/spacy/util.py\", line 1703, in _pipe\n    yield from proc.pipe(docs, **kwargs)\n  File \"spacy/pipeline/pipe.pyx\", line 60, in pipe\n  File \"/lustre/home/almusawiaf/anaconda3/envs/envBoW3/lib/python3.10/site-packages/spacy/util.py\", line 1722, in raise_error\n    raise e\n  File \"spacy/pipeline/pipe.pyx\", line 57, in spacy.pipeline.pipe.Pipe.pipe\n  File \"/lustre/home/almusawiaf/anaconda3/envs/envBoW3/lib/python3.10/site-packages/PyRuSH/PyRuSHSentencizer.py\", line 53, in __call__\n    cset_annotations([doc], tags)\n  File \"PyRuSH/StaticSentencizerFun.pyx\", line 48, in PyRuSH.StaticSentencizerFun.cset_annotations\n  File \"PyRuSH/StaticSentencizerFun.pyx\", line 56, in PyRuSH.StaticSentencizerFun.cset_annotations\n  File \"spacy/tokens/token.pyx\", line 515, in spacy.tokens.token.Token.sent_start.__set__\n  File \"spacy/tokens/token.pyx\", line 535, in spacy.tokens.token.Token.is_sent_start.__set__\nValueError: [E043] Refusing to write to token.sent_start if its document is parsed, because this may cause inconsistent state.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/text/ALL_first_last.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     44\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([char \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(x)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m char\u001b[38;5;241m.\u001b[39misdigit()]))\n\u001b[0;32m---> 45\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLEAN_TEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTEXT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m     48\u001b[0m bow_df \u001b[38;5;241m=\u001b[39m create_bow(df, vectorizer)\n",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m, in \u001b[0;36mparallel_preprocessing\u001b[0;34m(texts, n_jobs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparallel_preprocessing\u001b[39m(texts, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m---> 31\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Use the medspaCy enhanced nlp pipeline\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n",
      "File \u001b[0;32m~/anaconda3/envs/envBoW3/lib/python3.10/site-packages/spacy/language.py:1618\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[1;32m   1617\u001b[0m         docs \u001b[38;5;241m=\u001b[39m pipe(docs)\n\u001b[0;32m-> 1618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "File \u001b[0;32m~/anaconda3/envs/envBoW3/lib/python3.10/site-packages/spacy/language.py:1707\u001b[0m, in \u001b[0;36mLanguage._multiprocessing_pipe\u001b[0;34m(self, texts, pipes, n_process, batch_size)\u001b[0m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m byte_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1706\u001b[0m     error \u001b[38;5;241m=\u001b[39m srsly\u001b[38;5;241m.\u001b[39mmsgpack_loads(byte_error)\n\u001b[0;32m-> 1707\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_error_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;167;43;01mValueError\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mErrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mE871\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1711\u001b[0m     \u001b[38;5;66;03m# tell `sender` that one batch was consumed.\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m     sender\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/envBoW3/lib/python3.10/site-packages/spacy/util.py:1722\u001b[0m, in \u001b[0;36mraise_error\u001b[0;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraise_error\u001b[39m(proc_name, proc, docs, e):\n\u001b[0;32m-> 1722\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mValueError\u001b[0m: [E871] Error encountered in nlp.pipe with multiprocessing:\n\nTraceback (most recent call last):\n  File \"/lustre/home/almusawiaf/anaconda3/envs/envBoW3/lib/python3.10/site-packages/spacy/language.py\", line 2364, in _apply_pipes\n    byte_docs = [(doc.to_bytes(), doc._context, None) for doc in docs]\n  File \"/lustre/home/almusawiaf/anaconda3/envs/envBoW3/lib/python3.10/site-packages/spacy/language.py\", line 2364, in <listcomp>\n    byte_docs = [(doc.to_bytes(), doc._context, None) for doc in docs]\n  File \"/lustre/home/almusawiaf/anaconda3/envs/envBoW3/lib/python3.10/site-packages/spacy/util.py\", line 1703, in _pipe\n    yield from proc.pipe(docs, **kwargs)\n  File \"spacy/pipeline/pipe.pyx\", line 60, in pipe\n  File \"/lustre/home/almusawiaf/anaconda3/envs/envBoW3/lib/python3.10/site-packages/spacy/util.py\", line 1722, in raise_error\n    raise e\n  File \"spacy/pipeline/pipe.pyx\", line 57, in spacy.pipeline.pipe.Pipe.pipe\n  File \"/lustre/home/almusawiaf/anaconda3/envs/envBoW3/lib/python3.10/site-packages/PyRuSH/PyRuSHSentencizer.py\", line 53, in __call__\n    cset_annotations([doc], tags)\n  File \"PyRuSH/StaticSentencizerFun.pyx\", line 48, in PyRuSH.StaticSentencizerFun.cset_annotations\n  File \"PyRuSH/StaticSentencizerFun.pyx\", line 56, in PyRuSH.StaticSentencizerFun.cset_annotations\n  File \"spacy/tokens/token.pyx\", line 515, in spacy.tokens.token.Token.sent_start.__set__\n  File \"spacy/tokens/token.pyx\", line 535, in spacy.tokens.token.Token.is_sent_start.__set__\nValueError: [E043] Refusing to write to token.sent_start if its document is parsed, because this may cause inconsistent state.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import medspacy  # Import medspaCy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "nlp.add_pipe(\"medspacy_pyrush\") # Add medspaCy pipeline\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = ''.join([char for char in text if not char.isdigit()])  # Remove all digits\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation\n",
    "    text = text.strip()\n",
    "    doc = nlp(text) # Use the medspaCy enhanced nlp pipeline\n",
    "    tokens = [token.lemma_ for token in doc if token.text not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def parallel_preprocessing(texts, n_jobs=8):\n",
    "    docs = list(nlp.pipe(texts, n_process=n_jobs)) # Use the medspaCy enhanced nlp pipeline\n",
    "    return [\" \".join([token.lemma_ for token in doc]) for doc in docs]\n",
    "\n",
    "def create_bow(df, vectorizer):\n",
    "    X_bow = vectorizer.fit_transform(df[\"CLEAN_TEXT\"])\n",
    "    bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    bow_df.insert(0, \"HADM_ID\", df[\"HADM_ID\"])\n",
    "    return bow_df\n",
    "\n",
    "source_path = '../../../Data/unstructured'\n",
    "# df = pd.read_csv(f'{source_path}/summarized/ALL_first_last_SUMMARY_ONLY/1_t5_small2.csv').head(10)\n",
    "df = pd.read_csv(f'{source_path}/text/ALL_first_last.csv').head(10)\n",
    "\n",
    "df[\"TEXT\"] = df[\"TEXT\"].apply(lambda x: ''.join([char for char in str(x).lower() if not char.isdigit()]))\n",
    "df[\"CLEAN_TEXT\"] = parallel_preprocessing(df[\"TEXT\"].tolist(), n_jobs=8)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "bow_df = create_bow(df, vectorizer)\n",
    "\n",
    "bow_df.to_csv(f'{source_path}/emb/BoW/ALL_first_last_sci_sm_medspacy_2000.csv', index=False)\n",
    "\n",
    "print('Mission accomplished!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envBoW3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
