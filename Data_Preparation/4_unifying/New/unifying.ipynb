{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Unifying and Combining the final data**\n",
    "\n",
    "### 1. Unifying the data\n",
    "- The goal of this step is to unify the data from strucutured and unstructured data.\n",
    "    - Processing the structured data (XY)\n",
    "    - Processing the embedding of the unstructured data\n",
    "    - Saving...\n",
    "    \n",
    "### 2. Combining and preparing Y\n",
    "- The goal here is to combine the data so that we end with the following information:\n",
    "    - **Z**: base features\n",
    "    - **ZS**: base features + MeSH-based symptoms\n",
    "    - **T0**: original text\n",
    "    - **T1**: t5-small\n",
    "    - **T2**: bart large cnn\n",
    "    - **T3**: medical summarization\n",
    "\n",
    "    - **ZST0**: base + MeSH + original text\n",
    "    - **ZST1**: base + MeSH + t5-small\n",
    "    - **ZST2**: base + MeSH + bart large cnn\n",
    "    - **ZST3**: base + MeSH + medical summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unifying():\n",
    "    def __init__(self,  structured_path, unstructured_path, saving_path, the_model):\n",
    "        self.embedding_model = the_model\n",
    "        self.saving_path = f'{saving_path}/{self.embedding_model}'\n",
    "        \n",
    "        self.XB, self.XD, self.XL, self.XM, self.XP, self.XS, self.VIY, new_emb = self.extract_shared_data(structured_path, unstructured_path)\n",
    "        self.process_and_save_embeddings(new_emb)\n",
    "        self.concatenate_and_save()\n",
    "\n",
    "\n",
    "    def extract_shared_data(self, structured_path, unstructured_path):\n",
    "        \"\"\"\n",
    "        Extracts shared HADM_IDs from VIY and df_emb, then retrieves corresponding values from XB, XD, and LoS.\n",
    "        \n",
    "        Parameters:\n",
    "            VIY (np.ndarray): A 2D NumPy array with columns [HADM_ID, ICU_ID, LoS].\n",
    "            df_emb (pd.DataFrame): DataFrame containing HADM_ID and other columns.\n",
    "            XB (np.ndarray): Structured data corresponding to VIY.\n",
    "            XD (np.ndarray): Structured data corresponding to VIY.\n",
    "\n",
    "        Returns:\n",
    "            extracted_XB (np.ndarray): Filtered and sorted version of XB.\n",
    "            extracted_XD (np.ndarray): Filtered and sorted version of XD.\n",
    "            extracted_LoS (np.ndarray): Filtered and sorted Length of Stay values.\n",
    "            extracted_emb (pd.DataFrame): Filtered and sorted DataFrame containing embeddings.\n",
    "        \"\"\"\n",
    "        # --------------------------------------------------------------------------------------------------------\n",
    "        print('Loading the data')\n",
    "        # Reading reading reading...\n",
    "        # ... structured data\n",
    "        XB = self.load_pickle(f'{structured_path}/X_B.pkl')\n",
    "        XD = self.load_pickle(f'{structured_path}/X_D.pkl')\n",
    "        XL = self.load_pickle(f'{structured_path}/X_L.pkl')\n",
    "        XM = self.load_pickle(f'{structured_path}/X_M.pkl')\n",
    "        XP = self.load_pickle(f'{structured_path}/X_P.pkl')\n",
    "        XS = self.load_pickle(f'{structured_path}/X_S.pkl')\n",
    "        VIY = self.load_pickle(f'{structured_path}/VIY.pkl')\n",
    "\n",
    "        # ... unstructured data\n",
    "        df_emb = pd.read_csv(f'{unstructured_path}/merged_embeddings_{self.embedding_model}.csv')\n",
    "        \n",
    "        # --------------------------------------------------------------------------------------------------------\n",
    "        #                                   Extracting the data\n",
    "        # --------------------------------------------------------------------------------------------------------\n",
    "        # Convert VIY to DataFrame for easier operations\n",
    "        df_viy = pd.DataFrame(VIY, columns=['HADM_ID', 'ICU_ID', 'LoS'])\n",
    "\n",
    "        # Find common HADM_IDs\n",
    "        common_hadm_ids = np.intersect1d(df_viy['HADM_ID'].values, df_emb['HADM_ID'].values)\n",
    "\n",
    "        # Filter VIY and df_emb based on common HADM_IDs\n",
    "        filtered_viy = df_viy[df_viy['HADM_ID'].isin(common_hadm_ids)].sort_values(by='HADM_ID').reset_index(drop=True)\n",
    "        new_emb = df_emb[df_emb['HADM_ID'].isin(common_hadm_ids)].sort_values(by='HADM_ID').reset_index(drop=True)\n",
    "\n",
    "        # Create a mask to extract corresponding XB and XD values\n",
    "        mask = np.isin(VIY[:, 0], common_hadm_ids)\n",
    "\n",
    "        return XB[mask], XD[mask], XL[mask], XM[mask], XP[mask], XS[mask], VIY[mask, 2], new_emb\n",
    "    \n",
    "\n",
    "    def process_and_save_embeddings(self, merged_df):\n",
    "        \"\"\"\n",
    "        Extracts embedding columns from self.merged_df, converts text representations \n",
    "        of lists into NumPy arrays, and saves them as pickle files.\n",
    "        \"\"\"\n",
    "        print('Processing and saving the embedding ...')\n",
    "        embedding_columns = ['EMB_TEXT', \n",
    "                             'EMB_1_t5_small2_SUMMARY', \n",
    "                             'EMB_3_bart_large_cnn_SUMMARY', \n",
    "                             'EMB_4_medical_summarization_SUMMARY']\n",
    "        \n",
    "        for col in embedding_columns:\n",
    "            print(f\"Processing {col}...\")\n",
    "\n",
    "            # Convert string representation of lists to actual lists of floats\n",
    "            try:\n",
    "                processed_data = merged_df[col].apply(lambda x: np.array(ast.literal_eval(x), dtype=np.float32))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing column {col}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Stack into a single NumPy array (shape: [num_samples, embedding_dim])\n",
    "            embeddings_array = np.vstack(processed_data.to_numpy())\n",
    "\n",
    "            # Save the array using the provided function\n",
    "            self.save_list_as_pickle(embeddings_array, self.saving_path, col)\n",
    "            print(f\"Saved {col} with shape {embeddings_array.shape}\")\n",
    "    \n",
    "        \n",
    "    def classify_los_3_classes(self, los_list):\n",
    "        return [0 if los < 3 else 1 if 3 <= los <= 7 else 2 for los in los_list]\n",
    "\n",
    "    def concatenate_and_save(self):    \n",
    "        print('Reading the data ...')\n",
    "\n",
    "        T0 = load_pickle(f'{self.saving_path}/EMB_TEXT.pkl')\n",
    "        T1 = load_pickle(f'{self.saving_path}/EMB_1_t5_small2_SUMMARY.pkl')\n",
    "        T2 = load_pickle(f'{self.saving_path}/EMB_3_bart_large_cnn_SUMMARY.pkl')\n",
    "        T3 = load_pickle(f'{self.saving_path}/EMB_4_medical_summarization_SUMMARY.pkl')\n",
    "\n",
    "        Z = np.concatenate((self.XB, self.XD, self.XL, self.XM, self.XP), axis=1)\n",
    "        \n",
    "        ZS   = np.concatenate((Z, self.XS), axis=1)\n",
    "\n",
    "        ZST0 = np.concatenate((ZS, T0), axis=1)\n",
    "        ZST1 = np.concatenate((ZS, T1), axis=1)\n",
    "        ZST2 = np.concatenate((ZS, T2), axis=1)\n",
    "        ZST3 = np.concatenate((ZS, T3), axis=1)\n",
    "        \n",
    "        LoS    = self.VIY[:,2]\n",
    "        Visits = self.VIY[:,0]\n",
    "\n",
    "        Y = self.classify_los_3_classes(LoS)\n",
    "\n",
    "        print('|--- Saving ...')\n",
    "        self.save_list_as_pickle(Z,    self.saving_path, 'Z')\n",
    "        self.save_list_as_pickle(ZS,   self.saving_path, 'ZS')\n",
    "        self.save_list_as_pickle(T0,   self.saving_path, 'T0')\n",
    "        self.save_list_as_pickle(T1,   self.saving_path, 'T1')\n",
    "        self.save_list_as_pickle(T2,   self.saving_path, 'T2')\n",
    "        self.save_list_as_pickle(T3,   self.saving_path, 'T3')\n",
    "        self.save_list_as_pickle(ZST0, self.saving_path, 'ZST0')\n",
    "        self.save_list_as_pickle(ZST1, self.saving_path, 'ZST1')\n",
    "        self.save_list_as_pickle(ZST2, self.saving_path, 'ZST2')\n",
    "        self.save_list_as_pickle(ZST3, self.saving_path, 'ZST3')\n",
    "        self.save_list_as_pickle(Y,    self.saving_path, 'Y')\n",
    "\n",
    "    # FUNCTIONS\n",
    "    def save_list_as_pickle(self, L, given_path, file_name):\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(given_path):\n",
    "            os.makedirs(given_path)\n",
    "            print(f'\\tDirectory created: {given_path}')\n",
    "        \n",
    "        # Save the list as a pickle file\n",
    "        print(f'\\tSaving to {given_path}/{file_name}.pkl')\n",
    "        with open(os.path.join(given_path, f'{file_name}.pkl'), 'wb') as file:\n",
    "            pickle.dump(L, file)\n",
    "            \n",
    "    def load_pickle(self, thePath):\n",
    "        with open(thePath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data\n",
      "Processing and saving the embedding ...\n",
      "Processing EMB_TEXT...\n",
      "\tDirectory created: ../../../Data/XY2/clinicalbert\n",
      "\tSaving to ../../../Data/XY2/clinicalbert/EMB_TEXT.pkl\n",
      "Saved EMB_TEXT with shape (42142, 768)\n",
      "Processing EMB_1_t5_small2_SUMMARY...\n",
      "\tSaving to ../../../Data/XY2/clinicalbert/EMB_1_t5_small2_SUMMARY.pkl\n",
      "Saved EMB_1_t5_small2_SUMMARY with shape (42142, 768)\n",
      "Processing EMB_3_bart_large_cnn_SUMMARY...\n",
      "\tSaving to ../../../Data/XY2/clinicalbert/EMB_3_bart_large_cnn_SUMMARY.pkl\n",
      "Saved EMB_3_bart_large_cnn_SUMMARY with shape (42142, 768)\n",
      "Processing EMB_4_medical_summarization_SUMMARY...\n"
     ]
    }
   ],
   "source": [
    "structured_path = '../../../Data/structured'\n",
    "unstructured_path = '../../../Data/unstructured/emb'\n",
    "saving_path = '../../../Data/XY2'\n",
    "for the_model  in ['bioclinicalbert', 'clinicalbert', 'gatortron']:\n",
    "    _ = Unifying(structured_path, unstructured_path, saving_path, the_model)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envMeSH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
